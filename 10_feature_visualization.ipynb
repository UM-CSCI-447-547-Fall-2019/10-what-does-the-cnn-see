{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Feature Visualization\n",
    "\n",
    "## 1\n",
    "Just examining the kernels isn't very satisfying, because they tend to be small and numerous, and particularly difficult to understand when they're not being applied to the input image: In deeper layers of the network, we may be able to look at the kernels, but without considering previous feature maps in the image, we don't know what that kernel will *activate* on.  Perhaps one kernel might like to look for locations where cat ears are in close proximity to one another: the output of the previous layer might contain a kernel that activates on cat ears, but unless we know which kernel that is already, if we look at our current kernel we'll just know that it activates on when *something* is in close proximity.  \n",
    "\n",
    "However, all hope is not lost.  An alternative task that we might consider is to find the RGB image that causes a given kernel in a given layer to *activate as strongly as possible*.  **How might you approach solving this problem, i.e. finding this image?  What should be the objective function?  What are the things we get to adjust?**\n",
    "\n",
    "## 2 VGG\n",
    "To be honest, the CNNs that we've worked with so far aren't very interesting.  They've been trained on relatively small datasets, and have pretty few parameters.  They are, for all intents and purposes, toy examples, and their behaviour is pretty uninteresting.  Why don't we use bigger and more modern networks as teaching examples?  They take too long to train!  However, for real industrial scale applications, many interesting CNNs already exist and are available *pre-trained*, and we can query their behavior.  One notable example is the neural network VGG16, which has the following architecture:\n",
    "<img src=\"vgg.png\" />\n",
    "This is a so-called *deep network* because it has many layers.  It is also interesting because it doesn't do much fancy stuff (take advanced ML or Computer Vision if you want to know about fancy stuff), and the features that it extracts end up being relatively comprehensible.  Note that VGG was trained on (a subset of) the [ImageNet](http://www.image-net.org/) dataset, which contains 10M labelled images.  VGG was trained on more than a million of these and can recognize 1k different classes.  It's very large: 138M parameters and was trained for three weeks on an array of four Titan XP GPUs.  \n",
    "\n",
    "We have straightforward access to it via pytorch, specifically the torchvision package.  It might take you a bit of time to download the weights the first time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as tvm\n",
    "model = tvm.vgg16(pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have VGG, we'd like to create a mechanism for solving the optimization problem that we figured out above: For a given kernel, find the image that maximizes the mean of the feature map produced by that kernel.  The following class has that capability.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class FeatureVisualizer(object):\n",
    "    \"\"\"Class for visualizing features in the pretrained pytorch VGG model\"\"\"\n",
    "    def __init__(self,model):\n",
    "        \"\"\"Pass in the correct model\"\"\"\n",
    "        self.model = model  \n",
    "           \n",
    "    def create_feature_maximizer(self,layer_index,kernel_index,n_steps=100,gaussian_blur=0.1,imsize=224):\n",
    "        \"\"\"Finds an image that maximizes the activations induced by a given kernel\n",
    "        Arguments:\n",
    "           layer_index: the index of the layer in question\n",
    "           kernel_index: the index of the kernel in that layer\n",
    "           n_steps: number of optimization steps to take\n",
    "           gaussian_blur: how much to blur the resulting image to minimize high frequency noise\n",
    "           imsize: how big an image to make\n",
    "        Outputs: \n",
    "           img: the image that maximizes the given activation\n",
    "        \"\"\"\n",
    "        \n",
    "        # Instantiate a random image of the appropriate size and convert it to a variable\n",
    "        img = torch.from_numpy(np.uint8(np.random.uniform(-125, 125, (3,imsize, imsize)))/255)\n",
    "        img = img.to(torch.float32).detach().unsqueeze(0)\n",
    "        img = img.clone().detach().requires_grad_(True)\n",
    "\n",
    "        # Instantiate an optimizer, with the pixel values of the input image as the variable to be optimized\n",
    "        optimizer = torch.optim.Adam([img], lr=1e-1,weight_decay=1e-6)\n",
    "\n",
    "        # Optimize\n",
    "        for i in range(n_steps):\n",
    "            # Zero the gradient buffer\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Create a data structure to store intermediate network outputs\n",
    "            outputs = []\n",
    "            def hook_fn(module, input, output):\n",
    "                outputs.append(output)\n",
    "            \n",
    "            # Create a function that will store a given layer's output\n",
    "            hook = model.features[layer_index].register_forward_hook(hook_fn)\n",
    "            \n",
    "            # Run the model on the current image value\n",
    "            output = model(img)\n",
    "            \n",
    "            # Define the loss using the layer output\n",
    "            loss = -outputs[0][0,kernel_index].mean()\n",
    "            \n",
    "            # Print some stats\n",
    "            print(\"Negative Mean activation for layer:\",layer_index,\", kernel index\", kernel_index,\": \",loss.item())\n",
    "            \n",
    "            # Compute the gradient\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the pixel values\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Destroy the intermediate output function\n",
    "            hook.remove()\n",
    "            \n",
    "        # Convert img to w x h x channel, convert to numpy, remove batch dimension\n",
    "        img = np.moveaxis(img.detach().numpy().squeeze(),0,2)\n",
    "        \n",
    "        # Normalize to [0,1] and blur\n",
    "        img = gaussian_filter(np.dstack(([(img[:,:,i] - img[:,:,i].min())/(img[:,:,i].max() - img[:,:,i].min()) for i in range(3)])),gaussian_blur)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def get_mean_layer_activations(self,image_path,layer_index,imsize=224):\n",
    "        \"\"\"Computes the mean activations of a layer for a given image\n",
    "        Arguments:\n",
    "          image_path: the path to the image we want to run the network on\n",
    "          layer_index: the layer that we want to compute the mean activations for\n",
    "        Outputs:\n",
    "          mean_layer_activations: the mean of each feature map in a layer after being evaluated on an image\n",
    "        \"\"\"\n",
    "        # Load and normalize image to format expected by VGG network\n",
    "        loader = transforms.Compose([transforms.Resize(imsize),transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])])\n",
    "        \n",
    "        # Define data structure to hold intermediate network output\n",
    "        outputs= []\n",
    "        def hook_fn(module, input, output):\n",
    "            outputs.append(output)\n",
    "        \n",
    "        # Define a function to load the image\n",
    "        def image_loader(image_name):\n",
    "            \"\"\"load image, returns cuda tensor\"\"\"\n",
    "            image = Image.open(image_name)\n",
    "            image = loader(image).float()\n",
    "            image = Variable(image, requires_grad=True)\n",
    "            image = image.unsqueeze(0)\n",
    "            return image\n",
    "\n",
    "        # Load the image\n",
    "        img = image_loader(image_path)\n",
    "\n",
    "        # Run the model and save intermediate output\n",
    "        hook = self.model.features[layer_index].register_forward_hook(hook_fn)\n",
    "        output = self.model(img)\n",
    "        hook.remove()\n",
    "        mean_layer_activations = np.mean(outputs[0].detach().cpu().numpy().squeeze(),axis=(1,2))\n",
    "        return mean_layer_activations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to determine what layer to query.  We can do this by looking at the model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what type of image the first kernel in the first convolutional layer (after applying relu) is looking for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (10,10)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "layer = 1\n",
    "kernel = 0\n",
    "fv = FeatureVisualizer(model)\n",
    "img = fv.create_feature_maximizer(layer,kernel,imsize=128,n_steps=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IC10A Examining features at different layers\n",
    "Produce images like the ones above for several other kernels in this CNN layer.  What type of features does this layer seem to be focusing on.  Do the same for different layers.  What can you say about the complexity of the features being detected at deeper levels of the neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IC10B \n",
    "We can also examine what the network is focusing when extracting features *for a specific image*.  Download your own image of a common object (a cat, or a truck, or something simple), and use the *get_mean_layer_activations* method in the class given above to get the mean activations for a given layer as a function of the kernel index.  Identify which kernel for a given layer is most strongly activated by your particular image.  Then, generate the image that maximizes the activation for that layer and kernel.  What feature of your chosen image is the network looking at?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e.g.\n",
    "activations = fv.get_mean_layer_activations('cat.jpg',1)\n",
    "plt.plot(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
